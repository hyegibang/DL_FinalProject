{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import imageParse as imgP\n",
    "from audioSent_model import VAD_audio\n",
    "import audioSent_train as as_train\n",
    "import imageSent as imgS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAD_map = np.array([\n",
    "    [1,0.735,0.772], #0: 'happy'\n",
    "    [0.918,0.61,0.566], #1: 'funny'\n",
    "    [0.225,0.333,0.149], #2: 'sad'\n",
    "    [0.63,0.52,0.509], #3: 'tender'\n",
    "    [0.95,0.792,0.789], #4: 'exciting'\n",
    "    [0.122,0.83,0.604], #5: 'angry'\n",
    "    [0.062,0.952,0.528], #6: 'scary'\n",
    "])\n",
    "\n",
    "VAD_pd = pd.DataFrame(\n",
    "    columns = [\"valence\", \"arousal\", \"dominance\"], \n",
    "    data = VAD_map, \n",
    "    index = ['happy', 'funny', 'sad', 'tender', \n",
    "             'exciting', 'angry', 'scary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Sentiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_context_pd = pd.read_parquet(f\"../data/unbal_music_contexts.parquet\")\n",
    "train_embeddings = np.load(f\"../data/unbal_music_embeddings.npy\")\n",
    "test_context_pd = pd.read_parquet(f\"../data/bal_music_contexts.parquet\")\n",
    "test_embeddings = np.load(f\"../data/bal_music_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_train = tf.cast(train_embeddings.reshape(-1, 10, 128, 1)/128.0, \n",
    "                             dtype = tf.float32)\n",
    "label_train = tf.cast(train_context_pd[[\"valence\", \"arousal\", \"dominance\"]].to_numpy(), \n",
    "                        dtype = tf.float32)\n",
    "audio_test = tf.cast(test_embeddings.reshape(-1, 10, 128, 1)/128.0, \n",
    "                             dtype = tf.float32)\n",
    "label_test = tf.cast(test_context_pd[[\"valence\", \"arousal\", \"dominance\"]].to_numpy(), \n",
    "                        dtype = tf.float32)\n",
    "seg_label_train = train_context_pd[\"mood\"].to_numpy()\n",
    "seg_label_test = test_context_pd[\"mood\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "train_loss = 0.13785018026828766 \n",
      "\n",
      "Epoch: 2\n",
      "train_loss = 0.1010199636220932 \n",
      "\n",
      "Epoch: 3\n",
      "train_loss = 0.09076208621263504 \n",
      "\n",
      "Epoch: 4\n",
      "train_loss = 0.07875846326351166 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "numEpoch = 4\n",
    "model = VAD_audio()\n",
    "\n",
    "as_train.train_full(model, numEpoch, audio_train, label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Sentiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_X0, abs_Y0, abs_X1, abs_Y1 = imgP.parseData_Abs()\n",
    "abs_X0 = imgP.input_prep_fn(abs_X0)\n",
    "abs_X1 = imgP.input_prep_fn(abs_X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_X0, art_Y0, art_X1, art_Y1 = imgP.parseData_Art()\n",
    "art_X0 = imgP.input_prep_fn(art_X0)\n",
    "art_X1 = imgP.input_prep_fn(art_X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "28/28 [==============================] - 1s 11ms/step - mse_loss: 0.3888 - acc: 0.1339\n",
      "Epoch 2/5\n",
      "28/28 [==============================] - 0s 11ms/step - mse_loss: 0.2457 - acc: 0.1339\n",
      "Epoch 3/5\n",
      "28/28 [==============================] - 0s 12ms/step - mse_loss: 0.2514 - acc: 0.1339\n",
      "Epoch 4/5\n",
      "28/28 [==============================] - 0s 11ms/step - mse_loss: 0.2514 - acc: 0.1339\n",
      "Epoch 5/5\n",
      "28/28 [==============================] - 0s 12ms/step - mse_loss: 0.2514 - acc: 0.1339\n"
     ]
    }
   ],
   "source": [
    "abs_model = imgS.ImageSentModel(name='abs')\n",
    "abs_model.compile(optimizer=tf.keras.optimizers.Adam(5e-3), VAD_map=VAD_map)\n",
    "abs_Y0_VAD = VAD_pd.loc[abs_Y0]\n",
    "abs_Y1_VAD = VAD_pd.loc[abs_Y1]\n",
    "abs_model.fit(\n",
    "    (abs_X0, abs_Y0_VAD), abs_Y0_VAD,\n",
    "    epochs     = 5,\n",
    "    batch_size = 8,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "81/81 [==============================] - 1s 11ms/step - mse_loss: 0.3560 - acc: 0.1265\n",
      "Epoch 2/5\n",
      "81/81 [==============================] - 1s 11ms/step - mse_loss: 0.3650 - acc: 0.1265\n",
      "Epoch 3/5\n",
      "81/81 [==============================] - 1s 12ms/step - mse_loss: 0.3641 - acc: 0.1281\n",
      "Epoch 4/5\n",
      "81/81 [==============================] - 1s 11ms/step - mse_loss: 0.3632 - acc: 0.1265\n",
      "Epoch 5/5\n",
      "81/81 [==============================] - 1s 11ms/step - mse_loss: 0.3643 - acc: 0.1265\n"
     ]
    }
   ],
   "source": [
    "art_model = imgS.ImageSentModel(name='art')\n",
    "art_model.compile(optimizer=tf.keras.optimizers.Adam(5e-3), VAD_map=VAD_map)\n",
    "art_Y0_VAD = VAD_pd.loc[art_Y0]\n",
    "art_Y1_VAD = VAD_pd.loc[art_Y1]\n",
    "art_model.fit(\n",
    "    (art_X0, art_Y0_VAD), art_Y0_VAD,\n",
    "    epochs     = 5,\n",
    "    batch_size = 8,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching Images to Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[276 276 276 276 276 276 276 276 276 276 276 276 276 276 276 276 276 276\n",
      " 276 276 276 276 276 277 277 277 277 277 277 277 277 277 277 277 277 277\n",
      " 277 277 277 277 277 277 277 277 277 277 277 277 277 278 278 278 278 278\n",
      " 278 278 278 278 278 278 278 278 278 278 278 278 278 278 278 278 278 278\n",
      " 278 278 278 278 278 278 278 278 278 278 278 278 278 278 278 278 278 278\n",
      " 278 278 278 278 278 279 279 279 279 279 279 279 279 279 279 279 279 279\n",
      " 279 279 279 279 279 279 279 279 279 279 279 279 279 280 280 280 280 280\n",
      " 280 280 280 280 280 280 280 280 280 280 280 280 280 280 280 280 281 281\n",
      " 281 281 281 281 281 281 281 281 281 281 281 281 281 281 281 281 281 281\n",
      " 281 281 282 282 282 282 282 282 282 282 282 282 282 282 282 282 282 282\n",
      " 282 282 282 282]\n"
     ]
    }
   ],
   "source": [
    "print(seg_label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (dl3)",
   "language": "python",
   "name": "dl3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
